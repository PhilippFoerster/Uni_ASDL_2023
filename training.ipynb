{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import BCEWithLogitsLoss, MSELoss\n",
    "from transformers import RobertaTokenizerFast, RobertaConfig, RobertaModel, RobertaForSequenceClassification, get_linear_schedule_with_warmup\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from model import Model\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"microsoft/codebert-base\", do_lower_case=True)\n",
    "\n",
    "config = RobertaConfig.from_pretrained(\"microsoft/codebert-base\")\n",
    "config.num_labels = 1\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"microsoft/codebert-base\", config=config)\n",
    "model.to(device)\n",
    "#model = Model(model)\n",
    "#model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [\"<type>\", \"<specifier>\", \"<decl>\", \"<parameter_list>\", \"<decl_stmt>\", \"<comment>\", \"<directive>\", \"<parameter_list>\", \"<expr_stmt>\", \"<control>\", \"<argument_list>\", \"<parameter>\", \"<index>\",\n",
    "          \"<argument>\", \"<default>\", \"<call>\", \"<then>\", \"<label>\", \"<ternary>\", \"<condition>\", \"<init>\"]\n",
    "tokenizer.add_tokens(tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset.rename as rename\n",
    "tokenizer.add_tokens(rename.keywords)\n",
    "tokenizer.add_tokens(rename.standard_lib_funcs)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_freq_words = ['avctx','goto','uintt','sizeof','uint8t','avlog','codec','intt','\\\\n\\\\t','AVLOGERROR','\\\\n','pkt','dst','errp','opaque','arg1','channels','bufsize','ifdef','AVERROR','fprintf','linesize','samples','sps','d\\\\n','cpuenv','stderr','opcode','cpuR','arg2','opts','AVERRORINVALIDDATA','stride','getbits','t1','t0','avpkt','cpuT0','geterrno','memcpy','codecid','privdata','vdev','memset','pts','strcmp','ENOMEM','packet','AVCodecContext','0x','mbx','tcgtempfree','cpuir','localerr','gfree','streams','timebase','samplerate','TCGv','getbits1','idx','EINVAL','arg3','mbtype','modrm','movq','mv','amlappend','spapr','mby','tempblock','tb','regs','mm0','pixfmt','FIXME','qemu','errorsetg','bitrate','s\\\\n','scan8','DPRINTF','codectype','BlockDriverState','targetulong','irq','TCGvi','errorreport','0xff','extradata','acb','streamindex','avfreep','0xf','hci','mm1','blk','AVFrame','predictor','ramsize','cpumodel','avfree','pcstart','picttype','datasize','nbsectors','qxl','AVFormatContext','0x00','alac','CPUState','sizet','invalid','strlen','snprintf','REGa','framesize','chr','avmallocz','qiov','errno','t2','RRRS','mm2','xfer','tcgtempnew','aviorb','s\\'','nbstreams','memidx','pfl','kernelfilename','nbsamples','entries','AVNOPTSVALUE','tcgtempfreei','cpuA0','qdev','AVPacket','saddr','src2','tcgtempnewi','sectornum','dinfo','retval','RRRT','putbits','hdr','tmpz','csbase','insn','i8','gmalloc0','mm3','targetphysaddrt','efault','qemuirq','refcount','INTMAX','EIO','AVLOGDEBUG','AVRL','src1','tmp1','unlockuser','RRRR','tcggenmovii','unimplemented','Rn','AVLOGWARNING','fmt','motionval','cnt','FFMIN','mm4','xen9pdev','fdt','tcgconsti','xendev','exceptionindex','AVStream','timestamp','islit','avmalloc','MemoryRegion','putuser','cputmp2i','0x80','currentpictureptr','op1','mbheight','cputobe','dstFormat','illegalop','mbwidth']\n",
    "tokenizer.add_tokens(high_freq_words)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "\n",
    "def convert_examples_to_features(js):\n",
    "    code=' '.join(js['func'].split())\n",
    "    inputs = tokenizer(\n",
    "        code, \n",
    "        max_length=max_length, \n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "    return inputs, js['target']\n",
    "\n",
    "def convert_examples_to_features2(js):\n",
    "    code=' '.join(js['func'].split())\n",
    "    code_tokens = tokenizer.tokenize(code, return_tensors=\"pt\", max_length=510, padding=\"max_length\")\n",
    "    token_count = len(code_tokens)\n",
    "    code_tokens = code_tokens[:510]\n",
    "    source_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]\n",
    "    source_ids =  tokenizer.convert_tokens_to_ids(source_tokens)\n",
    "    source_ids = torch.tensor(source_ids)\n",
    "    return {\"input_ids\": source_ids, \"attention_mask\": source_ids.ne(1)}, js['target'], token_count\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_path, skip):\n",
    "        self.examples = []\n",
    "        with open(file_path) as f:\n",
    "            for line in f:\n",
    "                js=json.loads(line.strip())\n",
    "                input, target, token_count = convert_examples_to_features2(js)\n",
    "                if token_count > 800 and skip:\n",
    "                    continue\n",
    "                self.examples.append([input, target])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):       \n",
    "        return self.examples[i][0], self.examples[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset('dataset/train_renamed.jsonl', True)\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, pin_memory=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2732"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = CustomDataset('dataset/test_renamed.jsonl', False)\n",
    "batch_size = 4\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, pin_memory=True, shuffle=False)\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_epsilon = 1e-8\n",
    "learning_rate = 2e-5\n",
    "max_grad_norm = 1.0\n",
    "weight_decay = 0.00\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "epochs = 5\n",
    "max_steps = epochs * len(train_dataloader)\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=max_steps * 0.1, num_training_steps=max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_loss_steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 0.6640, acc 0.6072: 100%|██████████| 4967/4967 [16:28<00:00,  5.02it/s]\n",
      "100%|██████████| 683/683 [00:40<00:00, 16.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: loss 0.6967, acc 0.5732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss 0.6073, acc 0.6665: 100%|██████████| 4967/4967 [16:19<00:00,  5.07it/s]\n",
      "100%|██████████| 683/683 [00:41<00:00, 16.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: loss 0.7711, acc 0.6190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: loss 0.5573, acc 0.7497: 100%|██████████| 4967/4967 [16:11<00:00,  5.11it/s]\n",
      "100%|██████████| 683/683 [00:40<00:00, 16.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: loss 0.8628, acc 0.6307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: loss 0.5423, acc 0.8106: 100%|██████████| 4967/4967 [16:14<00:00,  5.09it/s]\n",
      "100%|██████████| 683/683 [00:42<00:00, 16.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: loss 1.2770, acc 0.6281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: loss 0.5109, acc 0.8572: 100%|██████████| 4967/4967 [16:14<00:00,  5.10it/s]\n",
      "100%|██████████| 683/683 [00:41<00:00, 16.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: loss 1.7605, acc 0.6234\n"
     ]
    }
   ],
   "source": [
    "stats = []\n",
    "for i, epoch in enumerate(range(epochs)):\n",
    "    #TRAINING\n",
    "    bar = tqdm(train_dataloader,total=len(train_dataloader))\n",
    "    bar.set_description(f\"Epoch {epoch}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    val_results = []\n",
    "    val_labels = []\n",
    "    for step, batch in enumerate(bar):\n",
    "        inputs = batch[0]\n",
    "        inputs = {key: value.squeeze(1).to(device) for key, value in inputs.items()}\n",
    "        labels = batch[1].to(device)\n",
    "        ids, mask = inputs['input_ids'], inputs['attention_mask']\n",
    "        #outputs = model(**inputs)\n",
    "        outputs = model(ids, mask)\n",
    "        #result = outputs[:,0]\n",
    "        result = outputs.logits[:,0]\n",
    "        loss = BCEWithLogitsLoss()(result, labels.float())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.mean().item()\n",
    "        val_results.append(result.cpu().detach().numpy())\n",
    "        val_labels.append(labels.cpu().numpy())\n",
    "        if step % update_loss_steps == 99:\n",
    "            results=np.concatenate(val_results,0)\n",
    "            labels=np.concatenate(val_labels,0)\n",
    "            preds=results>0.5\n",
    "            eval_acc=np.mean(labels==preds)\n",
    "            loss = total_loss / step\n",
    "            stats.append({'steps': (epoch*len(train_dataloader))+step, 'loss': loss, 'acc': eval_acc})\n",
    "            bar.set_description(f\"Epoch {epoch}: loss {loss:.4f}, acc {eval_acc:.4f}\")\n",
    "    \n",
    "    #EVAL\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        bar = tqdm(test_dataloader,total=len(test_dataloader))\n",
    "        total_loss = 0\n",
    "        val_results = []\n",
    "        val_labels = []\n",
    "        for step, batch in enumerate(bar):\n",
    "            inputs = batch[0]\n",
    "            inputs = {key: value.squeeze(1).to(device) for key, value in inputs.items()}\n",
    "            labels = batch[1].to(device)\n",
    "            ids, mask = inputs['input_ids'], inputs['attention_mask']\n",
    "            #outputs = model(**inputs)\n",
    "            outputs = model(ids, mask)\n",
    "            #result = outputs[:,0]\n",
    "            result = outputs.logits[:,0]\n",
    "            loss = BCEWithLogitsLoss()(result, labels.float())\n",
    "            total_loss += loss.mean().item()\n",
    "            val_results.append(result.cpu().detach().numpy())\n",
    "            val_labels.append(labels.cpu().numpy())\n",
    "    results=np.concatenate(val_results,0)\n",
    "    labels=np.concatenate(val_labels,0)\n",
    "    preds=results>0.5\n",
    "    eval_acc=np.mean(labels==preds)\n",
    "    loss = total_loss / step\n",
    "    time.sleep(1)\n",
    "    print(f\"Test set: loss {loss:.4f}, acc {eval_acc:.4f}\")\n",
    "    model.save_pretrained('Limit800/epoch'+str(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([s['steps'] for s in stats], [s['loss'] for s in stats])\n",
    "plt.plot([s['steps'] for s in stats], [s['acc'] for s in stats])\n",
    "plt.legend(['loss', 'acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\"Limit800/epoch4\", config=config)\n",
    "model.to(device)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RobertaConfig.from_pretrained(\"microsoft/codebert-base\")\n",
    "config.num_labels = 1\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"Limit800/epoch2\", config=config)\n",
    "model.to(device)\n",
    "#model = Model(model)\n",
    "#model.to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    bar = tqdm(test_dataloader,total=len(test_dataloader))\n",
    "    total_loss = 0\n",
    "    val_results = []\n",
    "    val_labels = []\n",
    "    for step, batch in enumerate(bar):\n",
    "        inputs = batch[0]\n",
    "        inputs = {key: value.squeeze(1).to(device) for key, value in inputs.items()}\n",
    "        labels = batch[1].to(device)\n",
    "        ids, mask = inputs['input_ids'], inputs['attention_mask']\n",
    "        #outputs = model(**inputs)\n",
    "        outputs = model(ids, mask)\n",
    "        #result = outputs[:,0]\n",
    "        result = outputs.logits[:,0]\n",
    "        loss = BCEWithLogitsLoss()(result, labels.float())\n",
    "        total_loss += loss.mean().item()\n",
    "        val_results.append(result.cpu().detach().numpy())\n",
    "        val_labels.append(labels.cpu().numpy())\n",
    "results=np.concatenate(val_results,0)\n",
    "labels=np.concatenate(val_labels,0)\n",
    "preds=results>0.5\n",
    "eval_acc=np.mean(labels==preds)\n",
    "loss = total_loss / step\n",
    "time.sleep(1)\n",
    "print(f\"loss {loss:.4f}, acc {eval_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
